{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: CRM Sales Analysis Pipeline\n",
    "\n",
    "**Author:** COMP647 Student  \n",
    "**Dataset:** CRM Sales Opportunities  \n",
    "**Source:** Kaggle - https://www.kaggle.com/datasets/innocentmfa/crm-sales-opportunities/data\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains a comprehensive analysis pipeline combining:\n",
    "1. **Data Exploration** - Initial dataset understanding\n",
    "2. **Data Preprocessing** - Cleaning and quality improvement\n",
    "3. **Data Integration** - Multi-table database merging\n",
    "4. **Exploratory Data Analysis** - Pattern discovery and insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure Environment\n",
    "\n",
    "All necessary libraries for the complete analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "import sys\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Initialize rich console\n",
    "console = Console()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ASSIGNMENT 2: CRM SALES OPPORTUNITIES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Environment configured successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing utilities \n",
    "from assignment_2 import (\n",
    "    assess_table_quality,\n",
    "    identify_placeholder_patterns,\n",
    "    analyze_duplicates,\n",
    "    remove_duplicates,\n",
    "    analyze_missing_patterns,\n",
    "    visualize_missing_patterns,\n",
    "    numerical_imputation,\n",
    "    categorical_imputation,\n",
    "    iqr_outlier_detection,\n",
    "    zscore_outlier_detection,\n",
    "    modified_zscore_outlier_detection,\n",
    "    visualize_outliers,\n",
    "    standardize_column_names,\n",
    "    convert_data_types\n",
    ")\n",
    "\n",
    "print(\"Preprocessing utilities imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: DATA EXPLORATION\n",
    "\n",
    "**Business Context:** First look at CRM Sales Opportunities Dataset  \n",
    "**Objective:** Understand dataset structure, content, and business domain  \n",
    "**Expected Outcome:** Foundation knowledge for preprocessing phase\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all the 5-table CRM database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRM Database Tables\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# file paths \n",
    "data_path = Path('data/raw/') \n",
    "\n",
    "# Load all tables\n",
    "try:\n",
    "    accounts = pd.read_csv(data_path / 'accounts.csv')\n",
    "    products = pd.read_csv(data_path / 'products.csv') \n",
    "    sales_pipeline = pd.read_csv(data_path / 'sales_pipeline.csv')\n",
    "    sales_teams = pd.read_csv(data_path / 'sales_teams.csv')\n",
    "    data_dictionary = pd.read_csv(data_path / 'data_dictionary.csv')\n",
    "    \n",
    "    print(\"All 5 CRM tables loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CRM DATABASE OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create summary dictionary\n",
    "tables_info = {\n",
    "    'accounts': accounts,\n",
    "    'products': products,\n",
    "    'sales_pipeline': sales_pipeline, \n",
    "    'sales_teams': sales_teams,\n",
    "    'data_dictionary': data_dictionary\n",
    "}\n",
    "\n",
    "print(f\"Business Domain: Customer Relationship Management (CRM)\")\n",
    "print(f\"Database Type: Relational (5 interconnected tables)\")\n",
    "print(f\"Primary Use Case: Sales opportunity tracking and analysis\")\n",
    "\n",
    "print(f\"\\nTable Summary:\")\n",
    "total_rows = 0\n",
    "total_columns = 0\n",
    "\n",
    "for name, df in tables_info.items():\n",
    "    print(f\"{name:<15}:    {df.shape[0]:>5,} rows Ã— {df.shape[1]:>2} columns\")\n",
    "    total_rows += df.shape[0]\n",
    "    total_columns += df.shape[1]\n",
    "\n",
    "print(f\"\\nDatabase Totals: {total_rows:,} total rows, {total_columns} total columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database business context - Table representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BUSINESS CONTEXT - TABLE PURPOSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display data dictionary first for context\n",
    "print(\"\\nDATA DICTIONARY - Field Definitions:\")\n",
    "print(\"-\" * 45)\n",
    "if not data_dictionary.empty:\n",
    "    print(data_dictionary.to_string(index=False))\n",
    "else:\n",
    "    print(\"Data dictionary is empty or not properly loaded\")\n",
    "\n",
    "print(f\"\\nTABLE RELATIONSHIPS & BUSINESS PURPOSE:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "business_context = {\n",
    "    'sales_pipeline': {\n",
    "        'purpose': 'PRIMARY TABLE - Individual sales opportunities/deals',\n",
    "        'business_value': 'Track each potential sale from engagement to close',\n",
    "        'key_fields': 'opportunity_id, deal_stage (Won/Lost), close_value'\n",
    "    },\n",
    "    'accounts': {\n",
    "        'purpose': 'Customer master data - Company information', \n",
    "        'business_value': 'Customer profiles, industry sectors, company size',\n",
    "        'key_fields': 'account, sector, revenue, employees'\n",
    "    },\n",
    "    'products': {\n",
    "        'purpose': 'Product catalog - What the company sells',\n",
    "        'business_value': 'Product portfolio with pricing information', \n",
    "        'key_fields': 'product, series, sales_price'\n",
    "    },\n",
    "    'sales_teams': {\n",
    "        'purpose': 'Sales organization - Who handles the sales',\n",
    "        'business_value': 'Sales team structure, territories, management hierarchy',\n",
    "        'key_fields': 'sales_agent, manager, regional_office'\n",
    "    },\n",
    "    'data_dictionary': {\n",
    "        'purpose': 'Documentation - Field definitions',\n",
    "        'business_value': 'Professional documentation of all database fields',\n",
    "        'key_fields': 'Table, Field, Description'\n",
    "    }\n",
    "}\n",
    "\n",
    "for table_name, info in business_context.items():\n",
    "    if table_name in tables_info:\n",
    "        df = tables_info[table_name]\n",
    "        print(f\"\\n{info['purpose']}\")\n",
    "        print(f\"Business Value: {info['business_value']}\")\n",
    "        print(f\"Key Fields: {info['key_fields']}\")\n",
    "        print(f\"Data Size: {df.shape[0]:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the data - Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE DATA PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table_name, df in tables_info.items():\n",
    "    print(f\"\\n{table_name.upper()} - Sample Records:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Show column names\n",
    "    print(f\"Columns ({len(df.columns)}): {list(df.columns)}\\n\")\n",
    "\n",
    "    if len(df) > 0:\n",
    "        # Display first 5 rows\n",
    "        sample_df = df.head(5)\n",
    "\n",
    "        # Use tabulate for pretty table printing\n",
    "        print(tabulate(sample_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "\n",
    "        if len(df.columns) > 10:\n",
    "            print(f\"\\n({len(df.columns)-10} more columns)\")\n",
    "    else:\n",
    "        print(\"Table is empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIAL DATA QUALITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table_name, df in tables_info.items():\n",
    "    print(f\"\\n{table_name.upper()} - Quality Overview:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        # Basic quality metrics\n",
    "        total_cells = df.size\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"Completeness: {100-missing_pct:.1f}% ({total_cells-missing_cells:,}/{total_cells:,} cells)\")\n",
    "        print(f\"Missing Values: {missing_cells:,} ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        # Check for asterisk placeholders \n",
    "        asterisk_count = 0\n",
    "        for col in df.columns:\n",
    "            asterisk_count += (df[col].astype(str) == '*').sum()\n",
    "        \n",
    "        if asterisk_count > 0:\n",
    "            print(f\"Asterisk Placeholders: {asterisk_count} (need conversion to NaN)\")\n",
    "        \n",
    "        # Check data types\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        object_cols = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        print(f\"Numeric Columns: {len(numeric_cols)}\")\n",
    "        print(f\"Text/Object Columns: {len(object_cols)}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"Duplicate Rows: {duplicates}\")\n",
    "    else:\n",
    "        print(\"Table is empty - cannot assess quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database relationship preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TABLE RELATIONSHIPS PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Expected foreign key relationships based on business logic\n",
    "relationships = [\n",
    "    {\n",
    "        'from': 'sales_pipeline',\n",
    "        'to': 'accounts', \n",
    "        'key': 'account',\n",
    "        'description': 'Each opportunity belongs to a customer account'\n",
    "    },\n",
    "    {\n",
    "        'from': 'sales_pipeline',\n",
    "        'to': 'products',\n",
    "        'key': 'product', \n",
    "        'description': 'Each opportunity is for a specific product'\n",
    "    },\n",
    "    {\n",
    "        'from': 'sales_pipeline',\n",
    "        'to': 'sales_teams',\n",
    "        'key': 'sales_agent',\n",
    "        'description': 'Each opportunity is handled by a sales agent'\n",
    "    },\n",
    "    {\n",
    "        'from': 'accounts',\n",
    "        'to': 'accounts',\n",
    "        'key': 'subsidiary_of',\n",
    "        'description': 'Some accounts are subsidiaries of other accounts'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Expected Relationships (to validate in preprocessing):\")\n",
    "for rel in relationships:\n",
    "    print(f\"{rel['from']} â†’ {rel['to']} via '{rel['key']}'\")\n",
    "    print(f\"{rel['description']}\")\n",
    "\n",
    "# Quick validation check\n",
    "print(f\"\\nRelationship Validation Preview:\")\n",
    "if len(sales_pipeline) > 0 and len(accounts) > 0:\n",
    "    # Check account relationship\n",
    "    pipeline_accounts = set(sales_pipeline['account'].dropna())\n",
    "    master_accounts = set(accounts['account'].dropna()) \n",
    "    unmatched_accounts = pipeline_accounts - master_accounts\n",
    "    \n",
    "    print(f\"Account Relationship:\")\n",
    "    print(f\"â€¢ Opportunities reference {len(pipeline_accounts)} unique accounts\")\n",
    "    print(f\"â€¢ Master account table has {len(master_accounts)} accounts\")\n",
    "    if unmatched_accounts:\n",
    "        print(f\"{len(unmatched_accounts)} opportunities reference accounts not in master table\")\n",
    "    else:\n",
    "        print(f\"All opportunity accounts found in master table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial business insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIAL BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(sales_pipeline) > 0:\n",
    "    print(\"SALES OPPORTUNITIES - Key Metrics:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Deal stage analysis\n",
    "    if 'deal_stage' in sales_pipeline.columns:\n",
    "        deal_stages = sales_pipeline['deal_stage'].value_counts()\n",
    "        print(f\"Deal Stages:\")\n",
    "        for stage, count in deal_stages.items():\n",
    "            pct = (count/len(sales_pipeline))*100\n",
    "            print(f\"â€¢ {stage}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Value analysis (if close_value exists)\n",
    "    if 'close_value' in sales_pipeline.columns:\n",
    "        values = sales_pipeline['close_value'].dropna()\n",
    "        if len(values) > 0:\n",
    "            print(f\"\\nDeal Values:\")\n",
    "            print(f\"â€¢ Total Pipeline Value: ${values.sum():,.0f}\")\n",
    "            print(f\"â€¢ Average Deal Size: ${values.mean():,.0f}\")\n",
    "            print(f\"â€¢ Median Deal Size: ${values.median():,.0f}\")\n",
    "\n",
    "if len(accounts) > 0 and 'sector' in accounts.columns:\n",
    "    print(f\"\\nCUSTOMER BASE - Industry Distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    sectors = accounts['sector'].value_counts().head(5)\n",
    "    for sector, count in sectors.items():\n",
    "        pct = (count/len(accounts))*100\n",
    "        print(f\"â€¢ {sector}: {count} companies ({pct:.1f}%)\")\n",
    "\n",
    "if len(products) > 0:\n",
    "    print(f\"\\nPRODUCT PORTFOLIO:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"â€¢ Total Products: {len(products)}\")\n",
    "    if 'series' in products.columns:\n",
    "        series_count = products['series'].nunique()\n",
    "        print(f\"â€¢ Product Series: {series_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables_clean dictionary from existing variables\n",
    "tables_clean = {\n",
    "    'accounts': accounts.copy(),\n",
    "    'products': products.copy(),\n",
    "    'sales_pipeline': sales_pipeline.copy(), \n",
    "    'sales_teams': sales_teams.copy(),\n",
    "    'data_dictionary': data_dictionary.copy()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: DATA PREPROCESSING\n",
    "\n",
    "**Business Context:** Multi-table CRM database preprocessing  \n",
    "**Objective:** Clean, standardize, and prepare data for analysis  \n",
    "**Expected Outcome:** Analysis-ready datasets with documented quality improvements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CRM tables\n",
    "print(\"\\nSTEP 1: DATA LOADING AND INITIAL ASSESSMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "data_path = Path('data/raw/')\n",
    "table_files = {\n",
    "    'accounts': 'accounts.csv',\n",
    "    'products': 'products.csv', \n",
    "    'sales_pipeline': 'sales_pipeline.csv',\n",
    "    'sales_teams': 'sales_teams.csv',\n",
    "    'data_dictionary': 'data_dictionary.csv'\n",
    "}\n",
    "\n",
    "tables = {}\n",
    "initial_quality_metrics = []\n",
    "\n",
    "print(\"Loading tables and assessing initial quality:\")\n",
    "\n",
    "for table_name, filename in table_files.items():\n",
    "    file_path = data_path / filename\n",
    "    \n",
    "    try:\n",
    "        # Load table with basic error handling\n",
    "        df = pd.read_csv(file_path)\n",
    "        tables[table_name] = df\n",
    "        \n",
    "        # Perform initial quality assessment\n",
    "        quality_metrics = assess_table_quality(df, table_name)\n",
    "        initial_quality_metrics.append(quality_metrics)\n",
    "        \n",
    "        print(f\"\\n{table_name.upper()}:\")\n",
    "        print(f\"  Dimensions: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "        print(f\"  Completeness: {quality_metrics['completeness_pct']:.1f}%\")\n",
    "        print(f\"  Missing cells: {quality_metrics['missing_cells']:,}\")\n",
    "        print(f\"  Duplicate rows: {quality_metrics['duplicate_rows']:,}\")\n",
    "        \n",
    "        # Identify placeholder patterns that indicate missing data\n",
    "        placeholders = identify_placeholder_patterns(df)\n",
    "        if placeholders:\n",
    "            print(f\"  Placeholder patterns found: {list(placeholders.keys())}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {filename} not found in {data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicates systematically\n",
    "print(\"\\nSTEP 2: SYSTEMATIC DUPLICATE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define business key columns for each table type\n",
    "business_keys = {\n",
    "    'accounts': ['account'], \n",
    "    'products': ['product'],  \n",
    "    'sales_pipeline': ['opportunity_id'], \n",
    "    'sales_teams': ['sales_agent'],  \n",
    "    'data_dictionary': ['table', 'field'] \n",
    "}\n",
    "\n",
    "duplicate_summary = []\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nAnalyzing duplicates in {table_name.upper()}:\")\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    key_columns = business_keys.get(table_name, [])\n",
    "    dup_analysis = analyze_duplicates(df, table_name, key_columns)\n",
    "    \n",
    "    print(f\"  Complete duplicate rows: {dup_analysis['complete_duplicates']}\")\n",
    "    print(f\"  Business key duplicates: {dup_analysis['key_duplicates']}\")\n",
    "    \n",
    "    # Show high-duplication columns (indicates potential data quality issues)\n",
    "    high_dup_columns = []\n",
    "    for col, info in dup_analysis['column_duplicates'].items():\n",
    "        if isinstance(info, dict) and info['rate_pct'] > 50:\n",
    "            high_dup_columns.append((col, info['rate_pct']))\n",
    "    \n",
    "    if high_dup_columns:\n",
    "        print(\"  High duplication columns:\")\n",
    "        for col, rate in high_dup_columns[:3]:  # Show top 3\n",
    "            print(f\"    {col}: {rate:.1f}% duplicated values\")\n",
    "    \n",
    "    # Remove duplicates based on table-specific strategy\n",
    "    if dup_analysis['complete_duplicates'] > 0:\n",
    "        df_cleaned, removed_count = remove_duplicates(df, strategy='complete')\n",
    "        print(f\"  Removed {removed_count} complete duplicate rows\")\n",
    "        tables[table_name] = df_cleaned\n",
    "    else:\n",
    "        print(\"  No complete duplicates to remove\")\n",
    "    \n",
    "    duplicate_summary.append({\n",
    "        'table': table_name,\n",
    "        'duplicates_removed': dup_analysis['complete_duplicates']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analysis of missing data patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns\n",
    "print(\"\\nSTEP 3: MISSING DATA PATTERN ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "missing_analyses = {}\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nAnalyzing missing patterns in {table_name.upper()}:\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_analysis = analyze_missing_patterns(df, table_name)\n",
    "    missing_analyses[table_name] = missing_analysis\n",
    "    \n",
    "    if missing_analysis['missing_summary']:\n",
    "        print(\"  Missing data by column:\")\n",
    "        for item in missing_analysis['missing_summary']:\n",
    "            print(f\"    {item['column']}: {item['missing_count']:,} ({item['missing_percentage']:.1f}%)\")\n",
    "        \n",
    "        # Analyze missing data correlations\n",
    "        if missing_analysis['missing_correlation'] is not None:\n",
    "            corr_matrix = missing_analysis['missing_correlation']\n",
    "            \n",
    "            # Find high correlations between missing patterns\n",
    "            high_correlations = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    corr_val = corr_matrix.iloc[i, j]\n",
    "                    if abs(corr_val) > 0.5:  # Strong correlation threshold\n",
    "                        high_correlations.append((\n",
    "                            corr_matrix.columns[i],\n",
    "                            corr_matrix.columns[j], \n",
    "                            corr_val\n",
    "                        ))\n",
    "            \n",
    "            if high_correlations:\n",
    "                print(\"  Strong missing data correlations:\")\n",
    "                for col1, col2, corr in high_correlations:\n",
    "                    print(f\"    {col1} â†” {col2}: {corr:.3f}\")\n",
    "        \n",
    "        # Create missing data visualizations\n",
    "        visualize_missing_patterns(df, table_name)\n",
    "    \n",
    "    else:\n",
    "        print(\"  No missing data detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Apply imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply intelligent imputation\n",
    "print(\"\\nSTEP 4: INTELLIGENT MISSING DATA IMPUTATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "imputation_log = {}\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nApplying imputation strategies to {table_name.upper()}:\")\n",
    "    \n",
    "    df_imputed = df.copy()\n",
    "    table_imputation_log = []\n",
    "    \n",
    "    # Get missing data analysis for this table\n",
    "    missing_info = missing_analyses.get(table_name, {})\n",
    "    missing_summary = missing_info.get('missing_summary', [])\n",
    "    \n",
    "    if not missing_summary:\n",
    "        print(\"  No missing data to impute\")\n",
    "        continue\n",
    "    \n",
    "    for missing_item in missing_summary:\n",
    "        column = missing_item['column']\n",
    "        missing_count = missing_item['missing_count']\n",
    "        data_type = missing_item['data_type']\n",
    "        \n",
    "        print(f\"\\n  Processing {column} ({missing_count:,} missing values):\")\n",
    "        \n",
    "        original_missing = df_imputed[column].isnull().sum()\n",
    "        \n",
    "        # Choose imputation strategy based on data type and business context\n",
    "        if table_name == 'accounts':\n",
    "            if column == 'subsidiary_of':\n",
    "                # Logic: Most companies are independent\n",
    "                df_imputed[column] = categorical_imputation(\n",
    "                    df_imputed[column], strategy='custom', custom_value='Independent'\n",
    "                )\n",
    "                strategy_used = \"Business logic: Independent companies\"\n",
    "            \n",
    "            elif column in ['revenue', 'employees'] and 'sector' in df_imputed.columns:\n",
    "                # Group-based numerical imputation\n",
    "                df_imputed[column] = numerical_imputation(\n",
    "                    df_imputed[column], strategy='group_median', \n",
    "                    group_by='sector', group_data=df_imputed\n",
    "                )\n",
    "                strategy_used = \"Sector-based median imputation\"\n",
    "            \n",
    "            else:\n",
    "                # Default numerical or categorical imputation\n",
    "                if pd.api.types.is_numeric_dtype(df_imputed[column]):\n",
    "                    df_imputed[column] = numerical_imputation(\n",
    "                        df_imputed[column], strategy='median'\n",
    "                    )\n",
    "                    strategy_used = \"Median imputation\"\n",
    "                else:\n",
    "                    df_imputed[column] = categorical_imputation(\n",
    "                        df_imputed[column], strategy='mode'\n",
    "                    )\n",
    "                    strategy_used = \"Mode imputation\"\n",
    "        \n",
    "        elif table_name == 'sales_pipeline':\n",
    "            if column == 'close_value' and 'deal_stage' in df_imputed.columns:\n",
    "                # Business logic: Only Won deals should have close values\n",
    "                won_median = df_imputed[df_imputed['deal_stage'] == 'Won']['close_value'].median()\n",
    "                \n",
    "                won_missing_mask = ((df_imputed['deal_stage'] == 'Won') & \n",
    "                                  df_imputed['close_value'].isnull())\n",
    "                \n",
    "                if won_missing_mask.any():\n",
    "                    df_imputed.loc[won_missing_mask, 'close_value'] = won_median\n",
    "                    strategy_used = f\"Won deals imputed with median: ${won_median:,.0f}\"\n",
    "                else:\n",
    "                    # impute Engaging deals with Won median\n",
    "                    engaging_missing_mask = ((df_imputed['deal_stage'] == 'Engaging') & \n",
    "                                           df_imputed['close_value'].isnull())\n",
    "                    if engaging_missing_mask.any():\n",
    "                        df_imputed.loc[engaging_missing_mask, 'close_value'] = won_median\n",
    "                        strategy_used = f\"Engaging deals estimated with Won median: ${won_median:,.0f}\"\n",
    "                    else:\n",
    "                        strategy_used = \"No Won deals missing close_value\"\n",
    "            \n",
    "            elif column in ['engage_date', 'close_date']:\n",
    "                # Time series imputation using interpolation\n",
    "                df_imputed[column] = pd.to_datetime(df_imputed[column], errors='coerce')\n",
    "                df_imputed[column] = numerical_imputation(\n",
    "                    df_imputed[column], strategy='interpolate'\n",
    "                )\n",
    "                strategy_used = \"Temporal interpolation\"\n",
    "            \n",
    "            elif column == 'account':\n",
    "                # Use most frequent account for missing values\n",
    "                df_imputed[column] = categorical_imputation(\n",
    "                    df_imputed[column], strategy='mode'\n",
    "                )\n",
    "                strategy_used = \"Most frequent account\"\n",
    "            \n",
    "            else:\n",
    "                # Default strategy based on data type\n",
    "                if pd.api.types.is_numeric_dtype(df_imputed[column]):\n",
    "                    df_imputed[column] = numerical_imputation(\n",
    "                        df_imputed[column], strategy='median'\n",
    "                    )\n",
    "                    strategy_used = \"Median imputation\"\n",
    "                else:\n",
    "                    df_imputed[column] = categorical_imputation(\n",
    "                        df_imputed[column], strategy='mode'\n",
    "                    )\n",
    "                    strategy_used = \"Mode imputation\"\n",
    "        \n",
    "        else:\n",
    "            # Generic imputation for other tables\n",
    "            if pd.api.types.is_numeric_dtype(df_imputed[column]):\n",
    "                df_imputed[column] = numerical_imputation(\n",
    "                    df_imputed[column], strategy='median'\n",
    "                )\n",
    "                strategy_used = \"Median imputation\"\n",
    "            else:\n",
    "                df_imputed[column] = categorical_imputation(\n",
    "                    df_imputed[column], strategy='mode'\n",
    "                )\n",
    "                strategy_used = \"Mode imputation\"\n",
    "        \n",
    "        # Record imputation results\n",
    "        final_missing = df_imputed[column].isnull().sum()\n",
    "        values_imputed = original_missing - final_missing\n",
    "        \n",
    "        print(f\"    Strategy: {strategy_used}\")\n",
    "        print(f\"    Values imputed: {values_imputed:,}\")\n",
    "        print(f\"    Remaining missing: {final_missing:,}\")\n",
    "        \n",
    "        table_imputation_log.append({\n",
    "            'column': column,\n",
    "            'original_missing': original_missing,\n",
    "            'values_imputed': values_imputed,\n",
    "            'final_missing': final_missing,\n",
    "            'strategy': strategy_used\n",
    "        })\n",
    "    \n",
    "    tables[table_name] = df_imputed\n",
    "    imputation_log[table_name] = table_imputation_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Detect and analyze outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and analyze outliers\n",
    "print(\"\\nSTEP 5: COMPREHENSIVE OUTLIER DETECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "outlier_analyses = {}\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nOutlier analysis for {table_name.upper()}:\")\n",
    "    \n",
    "    # Focus on numerical columns for outlier detection\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_cols) == 0:\n",
    "        print(\"  No numerical columns for outlier analysis\")\n",
    "        continue\n",
    "    \n",
    "    table_outlier_results = {}\n",
    "    \n",
    "    for column in numerical_cols:\n",
    "        if df[column].notna().sum() < 10:  \n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Analyzing outliers in {column}:\")\n",
    "        \n",
    "        # Apply multiple outlier detection methods\n",
    "        methods_results = {}\n",
    "        \n",
    "        # IQR method \n",
    "        iqr_results = iqr_outlier_detection(df[column])\n",
    "        methods_results['IQR'] = iqr_results\n",
    "        \n",
    "        # Z-score method \n",
    "        zscore_results = zscore_outlier_detection(df[column])\n",
    "        methods_results['Z-Score'] = zscore_results\n",
    "        \n",
    "        # Modified Z-score \n",
    "        modified_zscore_results = modified_zscore_outlier_detection(df[column])\n",
    "        methods_results['Modified Z-Score'] = modified_zscore_results\n",
    "        \n",
    "        # Report results for each method\n",
    "        for method_name, results in methods_results.items():\n",
    "            outlier_count = results['outlier_count']\n",
    "            outlier_pct = (outlier_count / df[column].notna().sum()) * 100\n",
    "            print(f\"    {method_name}: {outlier_count} outliers ({outlier_pct:.1f}%)\")\n",
    "        \n",
    "        # Business context analysis and recommendations\n",
    "        print(f\"    Business context analysis:\")\n",
    "        \n",
    "        if table_name == 'accounts' and column == 'revenue':\n",
    "            print(\"      â†’ Large revenue outliers expected (Fortune 500 companies)\")\n",
    "            treatment_recommendation = \"retain\"\n",
    "        \n",
    "        elif table_name == 'accounts' and column == 'employees':\n",
    "            outlier_pct = (iqr_results['outlier_count'] / df[column].notna().sum()) * 100\n",
    "            if outlier_pct > 5:\n",
    "                print(f\"      â†’ High outlier rate ({outlier_pct:.1f}%) may indicate data quality issues\")\n",
    "                treatment_recommendation = \"investigate\"\n",
    "            else:\n",
    "                print(\"      â†’ Normal outlier rate for employee counts\")\n",
    "                treatment_recommendation = \"retain\"\n",
    "        \n",
    "        elif table_name == 'sales_pipeline' and column == 'close_value':\n",
    "            print(\"      â†’ High-value deals may be legitimate enterprise sales\")\n",
    "            treatment_recommendation = \"review\"\n",
    "        \n",
    "        else:\n",
    "            # Generic recommendation based on outlier percentage\n",
    "            outlier_pct = (iqr_results['outlier_count'] / df[column].notna().sum()) * 100\n",
    "            if outlier_pct > 10:\n",
    "                print(\"      â†’ High outlier rate\")\n",
    "                treatment_recommendation = \"investigate\"\n",
    "            elif outlier_pct > 5:\n",
    "                print(\"      â†’ Moderate outlier rate\")\n",
    "                treatment_recommendation = \"review\"\n",
    "            else:\n",
    "                print(\"      â†’ Normal outlier rate\")\n",
    "                treatment_recommendation = \"retain\"\n",
    "        \n",
    "        # Create comprehensive visualizations\n",
    "        visualize_outliers(df, column, methods_results)\n",
    "        \n",
    "        table_outlier_results[column] = {\n",
    "            'methods_results': methods_results,\n",
    "            'treatment_recommendation': treatment_recommendation\n",
    "        }\n",
    "    \n",
    "    outlier_analyses[table_name] = table_outlier_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data transformations\n",
    "print(\"\\nSTEP 6: DATA TRANSFORMATION AND STANDARDIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "transformation_log = {}\n",
    "\n",
    "# Define data type mappings based on business understanding\n",
    "type_mappings = {\n",
    "    'sales_pipeline': {\n",
    "        'engage_date': 'datetime64',\n",
    "        'close_date': 'datetime64',\n",
    "        'close_value': 'float64'\n",
    "    },\n",
    "    'accounts': {\n",
    "        'year_established': 'int64',\n",
    "        'revenue': 'float64',\n",
    "        'employees': 'int64'\n",
    "    },\n",
    "    'products': {\n",
    "        'sales_price': 'float64'\n",
    "    }\n",
    "}\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nTransforming {table_name.upper()}:\")\n",
    "    \n",
    "    # Standardize column names\n",
    "    df_transformed, name_changes = standardize_column_names(df)\n",
    "    \n",
    "    if name_changes:\n",
    "        print(\"  Column names standardized (lowercase, underscores)\")\n",
    "    else:\n",
    "        print(\"  Column names already standardized\")\n",
    "    \n",
    "    # Apply data type conversions\n",
    "    table_type_mappings = type_mappings.get(table_name, {})\n",
    "    if table_type_mappings:\n",
    "        df_transformed, conversion_log = convert_data_types(\n",
    "            df_transformed, table_type_mappings\n",
    "        )\n",
    "        \n",
    "        print(\"  Data type conversions:\")\n",
    "        for column, log_entry in conversion_log.items():\n",
    "            if log_entry['success']:\n",
    "                print(f\"    {column}: {log_entry['from']} â†’ {log_entry['to']}\")\n",
    "            else:\n",
    "                print(f\"    {column}: Conversion failed - {log_entry['error']}\")\n",
    "    \n",
    "    tables[table_name] = df_transformed\n",
    "    transformation_log[table_name] = {\n",
    "        'column_names_changed': name_changes,\n",
    "        'type_conversions': conversion_log if table_type_mappings else {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final quality report and visualization\n",
    "print(\"\\nSTEP 7: COMPREHENSIVE QUALITY ASSESSMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate final quality metrics\n",
    "final_quality_metrics = []\n",
    "for table_name, df in tables.items():\n",
    "    final_metrics = assess_table_quality(df, table_name)\n",
    "    final_quality_metrics.append(final_metrics)\n",
    "\n",
    "# Compare initial and final metrics\n",
    "print(\"\\nPROCESSING RESULTS SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\n{'Table':<15} {'Original':<10} {'Final':<10} {'Improvement':<12} {'Missing Before':<15} {'Missing After':<15}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "total_missing_improvement = 0\n",
    "total_duplicates_removed = 0\n",
    "\n",
    "for initial, final in zip(initial_quality_metrics, final_quality_metrics):\n",
    "    table_name = initial['table_name']\n",
    "    \n",
    "    missing_improvement = initial['missing_cells'] - final['missing_cells']\n",
    "    duplicate_improvement = initial['duplicate_rows'] - final['duplicate_rows']\n",
    "    \n",
    "    completeness_improvement = final['completeness_pct'] - initial['completeness_pct']\n",
    "    \n",
    "    print(f\"{table_name:<15} {initial['completeness_pct']:>7.1f}% {final['completeness_pct']:>8.1f}% {completeness_improvement:>+8.1f}% {initial['missing_cells']:>12,} {final['missing_cells']:>12,}\")\n",
    "    \n",
    "    total_missing_improvement += missing_improvement\n",
    "    total_duplicates_removed += duplicate_improvement\n",
    "\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'TOTAL IMPROVEMENT':<15} {'':<10} {'':<10} {'':<12} {total_missing_improvement:>12,} {total_duplicates_removed:>12,}\")\n",
    "\n",
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Extract data for visualization\n",
    "table_names = [metrics['table_name'] for metrics in initial_quality_metrics]\n",
    "completeness_before = [metrics['completeness_pct'] for metrics in initial_quality_metrics]\n",
    "completeness_after = [metrics['completeness_pct'] for metrics in final_quality_metrics]\n",
    "missing_before = [metrics['missing_cells'] for metrics in initial_quality_metrics]\n",
    "missing_after = [metrics['missing_cells'] for metrics in final_quality_metrics]\n",
    "\n",
    "# Completeness improvement comparison\n",
    "x = np.arange(len(table_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,0].bar(x - width/2, completeness_before, width, label='Before', alpha=0.8)\n",
    "axes[0,0].bar(x + width/2, completeness_after, width, label='After', alpha=0.8)\n",
    "axes[0,0].set_xlabel('Tables')\n",
    "axes[0,0].set_ylabel('Completeness Percentage')\n",
    "axes[0,0].set_title('Data Completeness: Before vs After Preprocessing')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(table_names, rotation=45)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Missing values comparison\n",
    "missing_reduced = [before - after for before, after in zip(missing_before, missing_after)]\n",
    "axes[0,1].bar(table_names, missing_reduced, color='green', alpha=0.7)\n",
    "axes[0,1].set_xlabel('Tables')\n",
    "axes[0,1].set_ylabel('Missing Values Addressed')\n",
    "axes[0,1].set_title('Missing Values Successfully Processed')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Before and after missing values\n",
    "axes[1,0].bar(x - width/2, missing_before, width, label='Before', alpha=0.8)\n",
    "axes[1,0].bar(x + width/2, missing_after, width, label='After', alpha=0.8)\n",
    "axes[1,0].set_xlabel('Tables')\n",
    "axes[1,0].set_ylabel('Missing Values Count')\n",
    "axes[1,0].set_title('Missing Values: Before vs After')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(table_names, rotation=45)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall improvement summary\n",
    "if total_missing_improvement > 0:\n",
    "    axes[1,1].pie([total_missing_improvement], labels=[f'Missing Values\\nAddressed\\n({total_missing_improvement:,})'], \n",
    "                  autopct='%1.1f%%', startangle=90)\n",
    "    axes[1,1].set_title('Total Data Quality Improvements')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Data Quality\\nAlready High', \n",
    "                  horizontalalignment='center', verticalalignment='center',\n",
    "                  transform=axes[1,1].transAxes, fontsize=14)\n",
    "    axes[1,1].set_title('Data Quality Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed tables\n",
    "print(\"\\nSTEP 8: SAVING PREPROCESSED DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "output_path = Path('data/interim/')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving preprocessed tables to {output_path}:\")\n",
    "for table_name, df in tables.items():\n",
    "    output_file = output_path / f\"{table_name}_preprocessed.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  Saved {table_name}_preprocessed.csv ({df.shape[0]:,} rows Ã— {df.shape[1]} columns)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPREHENSIVE PREPROCESSING PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total missing values addressed: {total_missing_improvement:,}\")\n",
    "print(f\"Total duplicate rows removed: {total_duplicates_removed:,}\")\n",
    "print(f\"Processed tables saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: DATA INTEGRATION\n",
    "\n",
    "**Business Context:** Multi-table CRM database integration  \n",
    "**Objective:** Merge preprocessed tables into analysis-ready master dataset  \n",
    "**Expected Outcome:** Clean integrated dataset ready for EDA and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load preprocessing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 2: LOADING PREPROCESSED TABLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load all preprocessed tables\n",
    "data_path = Path('data/interim/')\n",
    "tables = {}\n",
    "\n",
    "table_files = {\n",
    "    'accounts': 'accounts_preprocessed.csv',\n",
    "    'products': 'products_preprocessed.csv', \n",
    "    'sales_pipeline': 'sales_pipeline_preprocessed.csv',\n",
    "    'sales_teams': 'sales_teams_preprocessed.csv',\n",
    "    'data_dictionary': 'data_dictionary_preprocessed.csv'\n",
    "}\n",
    "\n",
    "for table_name, filename in table_files.items():\n",
    "    file_path = data_path / filename\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        tables[table_name] = df\n",
    "        print(f\"Loaded {table_name}: {df.shape[0]:,} rows X {df.shape[1]} columns\")\n",
    "        \n",
    "        # Convert datetime columns back to datetime \n",
    "        if table_name == 'sales_pipeline':\n",
    "            df['engage_date'] = pd.to_datetime(df['engage_date'])\n",
    "            df['close_date'] = pd.to_datetime(df['close_date'])\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(tables)} tables for integration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Examine table structures and their relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 3: TABLE STRUCTURE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\n{table_name.upper()}:\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows X {df.shape[1]} columns\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show key columns for relationship identification\n",
    "    if table_name == 'sales_pipeline':\n",
    "        print(f\"  Key relationships: account, product, sales_agent\")\n",
    "    elif table_name == 'accounts':\n",
    "        print(f\"  Primary key: account\")\n",
    "    elif table_name == 'products':\n",
    "        print(f\"  Primary key: product\")\n",
    "    elif table_name == 'sales_teams':\n",
    "        print(f\"  Primary key: sales_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Validate foreign key relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 4: RELATIONSHIP VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def validate_foreign_keys(primary_table, foreign_table, primary_key, foreign_key, relationship_name):\n",
    "    \"\"\"Validate foreign key relationships between tables\"\"\"\n",
    "    \n",
    "    # Get unique values from both tables\n",
    "    primary_values = set(primary_table[primary_key].dropna())\n",
    "    foreign_values = set(foreign_table[foreign_key].dropna())\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    total_foreign = len(foreign_values)\n",
    "    matched = len(foreign_values.intersection(primary_values))\n",
    "    unmatched = len(foreign_values - primary_values)\n",
    "    match_rate = (matched / total_foreign * 100) if total_foreign > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{relationship_name}:\")\n",
    "    print(f\"Foreign key values: {total_foreign:,}\")\n",
    "    print(f\"Matched: {matched:,} ({match_rate:.1f}%)\")\n",
    "    print(f\"Unmatched: {unmatched:,}\")\n",
    "    \n",
    "    if unmatched > 0:\n",
    "        unmatched_values = list(foreign_values - primary_values)[:5]  # Show first 5\n",
    "        print(f\"  Sample unmatched: {unmatched_values}\")\n",
    "    \n",
    "    return {\n",
    "        'relationship': relationship_name,\n",
    "        'total_foreign': total_foreign,\n",
    "        'matched': matched,\n",
    "        'unmatched': unmatched,\n",
    "        'match_rate': match_rate\n",
    "    }\n",
    "\n",
    "# Validate all key relationships\n",
    "validation_results = []\n",
    "\n",
    "# sales_pipeline â†’ accounts\n",
    "result1 = validate_foreign_keys(\n",
    "    tables['accounts'], tables['sales_pipeline'], \n",
    "    'account', 'account', \n",
    "    \"Sales Pipeline â†’ Accounts\"\n",
    ")\n",
    "validation_results.append(result1)\n",
    "\n",
    "# sales_pipeline â†’ products  \n",
    "result2 = validate_foreign_keys(\n",
    "    tables['products'], tables['sales_pipeline'],\n",
    "    'product', 'product',\n",
    "    \"Sales Pipeline â†’ Products\"\n",
    ")\n",
    "validation_results.append(result2)\n",
    "\n",
    "# sales_pipeline â†’ sales_teams\n",
    "result3 = validate_foreign_keys(\n",
    "    tables['sales_teams'], tables['sales_pipeline'],\n",
    "    'sales_agent', 'sales_agent', \n",
    "    \"Sales Pipeline â†’ Sales Teams\"\n",
    ")\n",
    "validation_results.append(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Create integrated master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 5: MASTER DATASET INTEGRATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Start with sales_pipeline as the primary table (contains all opportunities)\n",
    "master_df = tables['sales_pipeline'].copy()\n",
    "print(f\"Starting with sales_pipeline: {master_df.shape[0]:,} rows\")\n",
    "\n",
    "# Left join with accounts (company information)\n",
    "print(\"Joining with accounts table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['accounts'], \n",
    "    on='account', \n",
    "    how='left', \n",
    "    suffixes=('', '_account')\n",
    ")\n",
    "print(f\"After joining accounts: {master_df.shape[0]:,} rows Ã— {master_df.shape[1]} columns\")\n",
    "\n",
    "# Left join with products (product details and pricing)\n",
    "print(\"Joining with products table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['products'],\n",
    "    on='product',\n",
    "    how='left',\n",
    "    suffixes=('', '_product')\n",
    ")\n",
    "print(f\"After joining products: {master_df.shape[0]:,} rows Ã— {master_df.shape[1]} columns\")\n",
    "\n",
    "# Left join with sales_teams (sales agent details)\n",
    "print(\"Joining with sales_teams table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['sales_teams'],\n",
    "    on='sales_agent', \n",
    "    how='left',\n",
    "    suffixes=('', '_team')\n",
    ")\n",
    "print(f\"After joining sales_teams: {master_df.shape[0]:,} rows Ã— {master_df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nFinal integrated dataset: {master_df.shape[0]:,} rows Ã— {master_df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Data quality assessment for integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 6: INTEGRATED DATASET QUALITY ASSESSMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Missing value analysis\n",
    "missing_summary = master_df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(master_df) * 100).round(1)\n",
    "\n",
    "print(\"Missing values in integrated dataset:\")\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_summary.index,\n",
    "    'Missing_Count': missing_summary.values,\n",
    "    'Missing_Pct': missing_pct.values\n",
    "}).query('Missing_Count > 0').sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    for _, row in missing_df.head(10).iterrows():\n",
    "        print(f\"{row['Column']}: {row['Missing_Count']:,} ({row['Missing_Pct']:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No missing values detected!\")\n",
    "\n",
    "# Data completeness by key business dimensions\n",
    "print(f\"\\nData completeness analysis:\")\n",
    "print(f\"Total opportunities: {len(master_df):,}\")\n",
    "\n",
    "# Check coverage from each joined table\n",
    "if 'revenue' in master_df.columns:\n",
    "    account_coverage = master_df['revenue'].notna().sum()\n",
    "    print(f\"Opportunities with account data: {account_coverage:,} ({account_coverage/len(master_df)*100:.1f}%)\")\n",
    "\n",
    "if 'sales_price' in master_df.columns:\n",
    "    product_coverage = master_df['sales_price'].notna().sum()\n",
    "    print(f\"Opportunities with product data: {product_coverage:,} ({product_coverage/len(master_df)*100:.1f}%)\")\n",
    "\n",
    "if 'manager' in master_df.columns:\n",
    "    team_coverage = master_df['manager'].notna().sum()\n",
    "    print(f\"Opportunities with sales team data: {team_coverage:,} ({team_coverage/len(master_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Integration validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 7: INTEGRATION VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Validate data integrity\n",
    "print(\"Data integrity checks:\")\n",
    "\n",
    "# Check for unexpected duplicates\n",
    "duplicate_opps = master_df['opportunity_id'].duplicated().sum()\n",
    "print(f\"Duplicate opportunities: {duplicate_opps} (should be 0)\")\n",
    "\n",
    "# Check join effectiveness  \n",
    "total_ops = len(master_df)\n",
    "\n",
    "if 'revenue' in master_df.columns:\n",
    "    account_join_success = (master_df['revenue'].notna().sum() / total_ops * 100)\n",
    "    print(f\"Account data joined: {account_join_success:.1f}% of opportunities\")\n",
    "\n",
    "if 'sales_price' in master_df.columns:\n",
    "    product_join_success = (master_df['sales_price'].notna().sum() / total_ops * 100) \n",
    "    print(f\"Product data joined: {product_join_success:.1f}% of opportunities\")\n",
    "\n",
    "if 'manager' in master_df.columns:\n",
    "    team_join_success = (master_df['manager'].notna().sum() / total_ops * 100)\n",
    "    print(f\"Team data joined: {team_join_success:.1f}% of opportunities\")\n",
    "\n",
    "# Business logic validation\n",
    "if 'deal_stage' in master_df.columns and 'close_value' in master_df.columns:\n",
    "    won_deals_with_value = master_df[\n",
    "        (master_df['deal_stage'] == 'Won') & \n",
    "        (master_df['close_value'].notna())\n",
    "    ].shape[0]\n",
    "    total_won_deals = (master_df['deal_stage'] == 'Won').sum()\n",
    "    \n",
    "    if total_won_deals > 0:\n",
    "        print(f\"Won deals with values: {won_deals_with_value}/{total_won_deals} ({won_deals_with_value/total_won_deals*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Display a sample of integrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 8: INTEGRATED DATASET PREVIEW\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Sample of integrated data:\")\n",
    "print(master_df.head())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"Shape: {master_df.shape}\")\n",
    "\n",
    "print(f\"\\nColumn summary:\")\n",
    "for col in master_df.columns:\n",
    "    non_null = master_df[col].notna().sum()\n",
    "    data_type = master_df[col].dtype\n",
    "    print(f\"{col}: {non_null:,}/{len(master_df):,} non-null ({data_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Save the integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 8: SAVING INTEGRATED DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create processed data directory\n",
    "output_path = Path('data/processed/')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save master integrated dataset\n",
    "master_filename = 'crm_master_dataset.csv'\n",
    "master_filepath = output_path / master_filename\n",
    "master_df.to_csv(master_filepath, index=False)\n",
    "print(f\"Saved {master_filename}: {master_df.shape[0]:,} rows X {master_df.shape[1]} columns\")\n",
    "\n",
    "# Save integration summary\n",
    "integration_summary = {\n",
    "    'metric': [\n",
    "        'Total Opportunities',\n",
    "        'Total Features',\n",
    "        'Original Tables Integrated',\n",
    "        'Account Data Coverage',\n",
    "        'Product Data Coverage', \n",
    "        'Sales Team Data Coverage',\n",
    "        'Complete Records'\n",
    "    ],\n",
    "    'value': [\n",
    "        len(master_df),\n",
    "        master_df.shape[1],\n",
    "        len(tables) - 1,  \n",
    "        master_df['revenue'].notna().sum() if 'revenue' in master_df.columns else 0,\n",
    "        master_df['sales_price'].notna().sum() if 'sales_price' in master_df.columns else 0,\n",
    "        master_df['manager'].notna().sum() if 'manager' in master_df.columns else 0,\n",
    "        master_df.dropna().shape[0]\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(integration_summary)\n",
    "summary_df.to_csv(output_path / 'integration_summary.csv', index=False)\n",
    "print(f\"Saved integration_summary.csv\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA INTEGRATION COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Master dataset: {master_df.shape[0]:,} opportunities Ã— {master_df.shape[1]} features\")\n",
    "print(f\"Integration success rates:\")\n",
    "for result in validation_results:\n",
    "    print(f\"{result['relationship']}: {result['match_rate']:.1f}%\")\n",
    "print(f\"Output location: {output_path}\")\n",
    "print(\"Ready for exploratory data analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "**Business Context:** Comprehensive analysis of integrated CRM sales data  \n",
    "**Objective:** Discover patterns, relationships, and insights in sales data  \n",
    "**Expected Outcome:** Data-driven insights for business decision making\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load the processed dataset (The combined one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master dataset\n",
    "df_ = pd.read_csv('data/processed/crm_master_dataset.csv')\n",
    "\n",
    "# Convert datetime columns\n",
    "df_['engage_date'] = pd.to_datetime(df_['engage_date'])\n",
    "df_['close_date'] = pd.to_datetime(df_['close_date'])\n",
    "\n",
    "print(f\"Dataset loaded: {df_.shape[0]:,} rows Ã— {df_.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Basic data exploration with dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Descriptive statistics for numerical featuress for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert close_value to thousands for better readability\n",
    "df_['close_value_k'] = (df_['close_value']/1000).round(1)\n",
    "\n",
    "df_.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Sample dataset (random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample for faster processing during EDA\n",
    "df = df_.sample(frac=0.30, random_state=42) # Randomly sample 30% of the dataset\n",
    "print(f\"Working with sample: {df.shape[0]:,} rows for EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Data type and identifying features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Identify numerical and categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(\"Numerical columns:\", numerical_columns.tolist()[:10])  # Show first 10\n",
    "print(\"Categorical columns:\", categorical_columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Correlation analysis with a target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar plot of Correlation with 'close_value_k'\n",
    "plt.figure(figsize=(20, 8))\n",
    "correlation_with_target = df[numerical_columns].corr()['close_value_k'].drop('close_value_k').sort_values(ascending=False)\n",
    "sns.barplot(x=correlation_with_target.index, y=correlation_with_target.values)\n",
    "plt.title('Correlation of Features with Deal Value (in thousands)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation with Deal Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - close_value shows perfect correlation (1.0) - expected since close_value_k is derived from it\n",
    "  - sales_price shows strong positive correlation (~0.7-0.8) - indicates pricing strategy aligns with deal outcomes\n",
    "  - revenue and employees show moderate positive correlation (~0.3-0.4) - larger companies tend to have bigger deals\n",
    "  - year_established shows weak/no correlation - company age doesn't significantly impact deal size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##### Correlation Analysis Insights\n",
    "  - **Strong predictor**: Sales price is the strongest predictor of deal value, suggesting price-to-value alignment\n",
    "  - **Company size matters**: Both revenue and employee count positively correlate with deal size\n",
    "  - **Age is irrelevant**: Company establishment year shows no meaningful correlation\n",
    "  - **Business implication**: Target larger companies with higher-priced products for maximum deal value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Deal stage distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal stage distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "deal_stage_counts = df['deal_stage'].value_counts()\n",
    "plt.pie(deal_stage_counts.values, labels=deal_stage_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Deal Stages')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df, x='deal_stage', order=deal_stage_counts.index)\n",
    "plt.title('Count of Opportunities by Deal Stage')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Win rate appears to be ~48-50%\n",
    "  - \"Lost\" and \"Won\" are roughly balanced\n",
    "  - Small percentage in \"Engaging\" stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##### Deal Pipeline Health Analysis\n",
    "  - **Balanced outcomes**: Nearly 50/50 win/loss ratio indicates realistic opportunity qualification\n",
    "  - **Pipeline efficiency**: Low \"Engaging\" percentage suggests quick deal progression\n",
    "  - **Potential concern**: High loss rate warrants investigation into loss reasons\n",
    "  - **Action**: Implement loss reason tracking to identify improvement areas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Pair plot for key numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features for pair plot\n",
    "key_features = ['close_value_k', 'revenue', 'employees', 'sales_price']\n",
    "pair_plot = sns.pairplot(df[key_features].dropna(), diag_kind='kde')\n",
    "pair_plot.fig.suptitle('Pair Plot of Key Business Metrics', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Strong linear relationship between sales_price and close_value_k\n",
    "  - Revenue and employees show positive correlation\n",
    "  - Some outliers in all metrics suggesting enterprise deals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##### Multi-dimensional Relationship Insights\n",
    "  - **Price-Value Alignment**: Linear relationship confirms pricing integrity\n",
    "  - **Company Size Cluster**: Revenue and employees cluster together - use for segmentation\n",
    "  - **Outlier Opportunities**: High-value outliers represent enterprise opportunities\n",
    "  - **Segmentation Strategy**: Consideration for separate strategies for SMB vs Enterprise based on these clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11: Sales performance over time analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare temporal data\n",
    "df['engage_year'] = df['engage_date'].dt.year\n",
    "df['engage_month'] = df['engage_date'].dt.month\n",
    "\n",
    "# Sales trend over years\n",
    "yearly_performance = df.groupby('engage_year').agg({\n",
    "    'opportunity_id': 'count',\n",
    "    'close_value_k': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=yearly_performance, x='engage_year', y='opportunity_id', marker='o')\n",
    "plt.title('Number of Opportunities by Year')\n",
    "plt.xlabel('Engagement Year')\n",
    "plt.ylabel('Number of Opportunities')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=yearly_performance, x='engage_year', y='close_value_k', marker='o', color='orange')\n",
    "plt.title('Average Deal Value by Year')\n",
    "plt.xlabel('Engagement Year') \n",
    "plt.ylabel('Average Deal Value (thousands)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Opportunity count increases over time (2016-2017)\n",
    "  - Average deal value remains relatively stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Performance Analysis\n",
    "  - **Growth trajectory**: Increasing opportunity volume shows business expansion\n",
    "  - **Value stability**: Consistent average deal values indicate pricing discipline\n",
    "  - **Concern**: Flat deal values despite volume growth - there might be a missing upsell opportunities.\n",
    "  - **Further action**: Implemention of value-based selling to increase average deal size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12: Revenue vs deal performance analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sector performance analysis\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Average deal value by sector\n",
    "sector_performance = df.groupby('sector').agg({\n",
    "    'close_value_k': 'mean',\n",
    "    'opportunity_id': 'count'\n",
    "}).reset_index().sort_values('close_value_k', ascending=True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=sector_performance, y='sector', x='close_value_k')\n",
    "plt.title('Average Deal Value by Sector')\n",
    "plt.xlabel('Average Deal Value (thousands)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=sector_performance, y='sector', x='opportunity_id')\n",
    "plt.title('Number of Opportunities by Sector')\n",
    "plt.xlabel('Number of Opportunities')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Significant variation in average deal value by sector\n",
    "  - Some sectors have high volume but low value, others opposite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sector Strategy Insights\n",
    "  - **High-value sectors**: Focus resources on sectors with highest average deal values\n",
    "  - **Volume vs Value trade-off**: Balance portfolio between high-volume and high-value sectors\n",
    "  - **Sector-specific pricing**: Consideration needed to differentiated pricing strategies by sector\n",
    "  - **Resource allocation**: Align sales team specialization with sector potential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 13: Deal value distribution by deal stage (KDE plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.kdeplot(data=df.dropna(subset=['close_value_k']), x='close_value_k', hue='deal_stage', fill=True, alpha=0.6)\n",
    "plt.title('Deal Value Distribution by Deal Stage')\n",
    "plt.xlabel('Deal Value (thousands)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### KDE Plot Insights:\n",
    "- Won deals show higher average values compared to lost deals\n",
    "- Lost deals have a concentration at lower values\n",
    "- Engaging deals span a wide range of potential values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deal Stage Value Distribution\n",
    "  - **Won deals skew higher**: Successful deals tend to be larger - quality over quantity\n",
    "  - **Lost deal concentration**: Most lost deals are small - possibly poor qualification\n",
    "  - **Engaging spread**: Wide range suggests uncertainty in pipeline valuation\n",
    "  - **Strategy**: Focus qualification efforts on higher-value opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 14: Company size analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers for better visualization\n",
    "df_clean = df[(df['employees'] < df['employees'].quantile(0.95)) & \n",
    "              (df['revenue'] < df['revenue'].quantile(0.95))].copy()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.scatterplot(data=df_clean, x='employees', y='revenue', hue='deal_stage', alpha=0.6, s=60)\n",
    "plt.title('Company Size vs Revenue by Deal Outcome')\n",
    "plt.xlabel('Number of Employees')\n",
    "plt.ylabel('Company Revenue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Findings:\n",
    "  - Clear positive correlation between company size and revenue\n",
    "  - Deal outcomes distributed across all company sizes\n",
    "\n",
    "  ##### Company Profile Analysis\n",
    "  - **Size-Revenue correlation**: Validates the data quality and company profiling\n",
    "  - **Universal opportunity**: Wins occur across all company sizes\n",
    "  - **No clear pattern**: Deal outcomes not strictly determined by company size\n",
    "  - **Implication**: Other factors (product fit, timing) may be more important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 15: Histogram - Deal value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deal size categories\n",
    "df['deal_size_category'] = pd.cut(df['close_value_k'], \n",
    "                                  bins=[0, 1, 5, 15, float('inf')], \n",
    "                                  labels=['Small', 'Medium', 'Large', 'Enterprise'])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['close_value_k'].dropna(), bins=50, kde=True)\n",
    "plt.title(\"Distribution of Deal Values\")\n",
    "plt.xlabel('Deal Value (thousands)')\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "sns.countplot(data=df, x='deal_size_category', order=['Small', 'Medium', 'Large', 'Enterprise'])\n",
    "plt.title(\"Count by Deal Size Category\")\n",
    "plt.xlabel('Deal Size Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deal Value Distribution Insights:\n",
    "- Most deals are in the small to medium range\n",
    "- Distribution is right-skewed with some high-value outliers\n",
    "- Enterprise deals are relatively rare but high-value\n",
    "\n",
    " ##### Deal Size Segmentation\n",
    "  - **Pareto principle**: Likely 80% of revenue from 20% of deals\n",
    "  - **Volume strategy needed**: Small deals dominate count - need efficiency\n",
    "  - **Enterprise focus**: Dedicate specialized resources to rare but valuable enterprise deals\n",
    "  - **Pricing strategy**: Considerations needed for tiered approach based on deal size categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 16: Box plot - Sales agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top sales agents by deal count\n",
    "top_agents = df['sales_agent'].value_counts().head(10).index\n",
    "df_top_agents = df[df['sales_agent'].isin(top_agents)]\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(data=df_top_agents, x='sales_agent', y='close_value_k')\n",
    "plt.title('Deal Value Distribution by Top Sales Agents')\n",
    "plt.xlabel('Sales Agent')\n",
    "plt.ylabel('Deal Value (thousands)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sales Agent Performance Insights:\n",
    "- Different agents show varying deal value distributions\n",
    "- Some agents consistently close higher-value deals\n",
    "- Agent performance varies significantly across the team\n",
    "\n",
    " ##### Sales Team Performance Analysis\n",
    "  - **Performance variance**: Wide distribution suggests skill/strategy differences\n",
    "  - **Top performers**: Identify and replicate strategies of high-value closers\n",
    "  - **Training opportunity**: Bottom performers need coaching or different territories\n",
    "  - **Commission structure**: Considerations for value-based incentives vs volume-based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 17: Regional office analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional performance\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "regional_performance = df.groupby('regional_office').agg({\n",
    "    'close_value_k': ['mean', 'count'],\n",
    "    'deal_stage': lambda x: (x == 'Won').mean()\n",
    "}).round(3)\n",
    "\n",
    "regional_performance.columns = ['avg_deal_value', 'total_deals', 'win_rate']\n",
    "regional_performance = regional_performance.reset_index().sort_values('avg_deal_value', ascending=True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=regional_performance, y='regional_office', x='avg_deal_value')\n",
    "plt.title('Average Deal Value by Regional Office')\n",
    "plt.xlabel('Average Deal Value (thousands)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=regional_performance, y='regional_office', x='win_rate')\n",
    "plt.title('Win Rate by Regional Office')\n",
    "plt.xlabel('Win Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Different regions show varying performance levels\n",
    "  - Win rates differ significantly by region\n",
    "\n",
    "##### Geographic Performance Insights\n",
    "  - **Regional disparities**: Significant performance differences require investigation\n",
    "  - **Best practices**: Study top-performing regions for replicable strategies\n",
    "  - **Market conditions**: Consider local market factors affecting performance\n",
    "  - **Resource reallocation**: Shift resources to high-potential regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 18: Product series performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product analysis\n",
    "product_performance = df.groupby('series').agg({\n",
    "    'close_value_k': 'mean',\n",
    "    'sales_price': 'mean',\n",
    "    'opportunity_id': 'count'\n",
    "}).reset_index().sort_values('close_value_k', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=product_performance, x='series', y='close_value_k')\n",
    "plt.title('Average Deal Value by Product Series')\n",
    "plt.xlabel('Product Series')\n",
    "plt.ylabel('Average Deal Value (thousands)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=df, x='sales_price', y='close_value_k', hue='series', alpha=0.6)\n",
    "plt.title('Sales Price vs Deal Value by Product Series')\n",
    "plt.xlabel('List Price')\n",
    "plt.ylabel('Deal Value (thousands)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 19:  Time series analysis - Sales cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sales cycle\n",
    "df['sales_cycle_days'] = (df['close_date'] - df['engage_date']).dt.days\n",
    "\n",
    "# Filter reasonable sales cycles\n",
    "df_cycle = df[(df['sales_cycle_days'] > 0) & (df['sales_cycle_days'] < 365)].copy()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_cycle['sales_cycle_days'], bins=30, kde=True)\n",
    "plt.title('Distribution of Sales Cycle Length')\n",
    "plt.xlabel('Sales Cycle (days)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df_cycle, x='deal_stage', y='sales_cycle_days')\n",
    "plt.title('Sales Cycle by Deal Outcome')\n",
    "plt.xlabel('Deal Stage')\n",
    "plt.ylabel('Sales Cycle (days)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 20: Geographic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Office location analysis\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "location_performance = df.groupby('office_location').agg({\n",
    "    'close_value_k': 'mean',\n",
    "    'opportunity_id': 'count'\n",
    "}).reset_index().sort_values('close_value_k', ascending=True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=location_performance, y='office_location', x='close_value_k')\n",
    "plt.title('Average Deal Value by Customer Location')\n",
    "plt.xlabel('Average Deal Value (thousands)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=location_performance, y='office_location', x='opportunity_id')\n",
    "plt.title('Number of Opportunities by Customer Location')\n",
    "plt.xlabel('Number of Opportunities')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 21: Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key numerical features for correlation matrix\n",
    "key_numerical_features = ['close_value_k', 'revenue', 'employees', 'sales_price', \n",
    "                         'year_established', 'sales_cycle_days']\n",
    "\n",
    "correlation_matrix = df[key_numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Key Business Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "  - Strong correlation between company metrics (revenue, employees)\n",
    "  - Sales price strongly correlates with deal value\n",
    "  - Sales cycle shows weak correlations\n",
    "\n",
    "##### Correlation Matrix Insights\n",
    "  - **Multicollinearity**: Revenue and employees highly correlated \n",
    "  - **Independent cycle**: Sales cycle length independent of other factors\n",
    "  - **Pricing power**: Strong price-value correlation confirms pricing strategy\n",
    "  - **Predictive features**: Focus on sales_price, revenue/employees for deal value prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 22: Deal value vs company characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company age analysis\n",
    "df['company_age'] = 2024 - df['year_established']\n",
    "df_age = df[(df['company_age'] > 0) & (df['company_age'] < 100)].copy()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=df_age, x='company_age', y='close_value_k', hue='deal_stage', alpha=0.6)\n",
    "plt.title('Deal Value vs Company Age')\n",
    "plt.xlabel('Company Age (years)')\n",
    "plt.ylabel('Deal Value (thousands)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Revenue per employee efficiency\n",
    "df['revenue_per_employee'] = df['revenue'] / df['employees']\n",
    "df_efficiency = df[df['revenue_per_employee'] < df['revenue_per_employee'].quantile(0.95)].copy()\n",
    "sns.scatterplot(data=df_efficiency, x='revenue_per_employee', y='close_value_k', \n",
    "                hue='deal_stage', alpha=0.6)\n",
    "plt.title('Deal Value vs Company Efficiency')\n",
    "plt.xlabel('Revenue per Employee')\n",
    "plt.ylabel('Deal Value (thousands)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 23: Win rate analysis by different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_win_rate(group):\n",
    "    return (group == 'Won').mean()\n",
    "\n",
    "# Win rates by different dimensions\n",
    "win_rates = pd.DataFrame({\n",
    "    'Sector': df.groupby('sector')['deal_stage'].apply(calculate_win_rate),\n",
    "    'Product_Series': df.groupby('series')['deal_stage'].apply(calculate_win_rate),\n",
    "    'Regional_Office': df.groupby('regional_office')['deal_stage'].apply(calculate_win_rate),\n",
    "    'Deal_Size': df.groupby('deal_size_category')['deal_stage'].apply(calculate_win_rate)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, (column, data) in enumerate(win_rates.items(), 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    data_clean = data.dropna().sort_values(ascending=True)\n",
    "    sns.barplot(x=data_clean.values, y=data_clean.index)\n",
    "    plt.title(f'Win Rate by {column.replace(\"_\", \" \")}')\n",
    "    plt.xlabel('Win Rate')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 24: Getting unique value in categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical column diversity\n",
    "categorical_cols_subset = ['sector', 'series', 'deal_stage', 'regional_office', 'office_location']\n",
    "unique_counts = df[categorical_cols_subset].nunique()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=unique_counts.index, y=unique_counts.values, hue=unique_counts.index, legend=False)\n",
    "plt.title('Unique Value Counts for Key Categorical Columns')\n",
    "plt.xlabel('Categorical Columns')\n",
    "plt.ylabel('Number of Unique Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Categorical Column Analysis:\")\n",
    "for col, count in unique_counts.items():\n",
    "    print(f\"{col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 25: Summary of insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY BUSINESS INSIGHTS FROM EDA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate key metrics\n",
    "total_opportunities = len(df)\n",
    "won_deals = len(df[df['deal_stage'] == 'Won'])\n",
    "win_rate = won_deals / total_opportunities\n",
    "avg_deal_value = df['close_value_k'].mean()\n",
    "avg_sales_cycle = df_cycle['sales_cycle_days'].mean()\n",
    "\n",
    "print(f\"OVERALL PERFORMANCE:\")\n",
    "print(f\"- Total Opportunities Analyzed: {total_opportunities:,}\")\n",
    "print(f\"- Overall Win Rate: {win_rate:.1%}\")\n",
    "print(f\"- Average Deal Value: ${avg_deal_value:.1f}K\")\n",
    "print(f\"- Average Sales Cycle: {avg_sales_cycle:.0f} days\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the EDA, here are some research that can be explored with supporting evidence from EDA: \n",
    "\n",
    "  1. What factors predicts the sucess rate for a deal (Won vs Lost)?\n",
    "\n",
    "  - Evidence: 48% win rate with variation across segments\n",
    "  - Approach: A classification model using company size, sector, product, agent\n",
    "  - Business value: Improve qualification and increase win rate\n",
    "\n",
    "  2. Are  we  able to predict deal value before engagement?\n",
    "\n",
    "  - Evidence: Strong correlations with company metrics and product pricing\n",
    "  - Approach: Regression model using company characteristics\n",
    "  - Business value: Better resource allocation and forecasting\n",
    "\n",
    "  3. What drives sales cycle length?\n",
    "\n",
    "  - Evidence: Sales cycle varies but shows weak correlation with other metrics\n",
    "  - Approach: Analyze cycle time by deal complexity, sector, agent\n",
    "  - Business value: Reduce cycle time and improve cash flow\n",
    "\n",
    "  4. Which customer segments offer highest ROI?\n",
    "\n",
    "  - Evidence: Sector and company size show different value patterns\n",
    "  - Approach: Segment analysis combining win rate, deal value, and cycle time\n",
    "  - Business value: Focus on most profitable segments\n",
    "\n",
    "  5. How can we optimize sales team performance?\n",
    "\n",
    "  - Evidence: High variance in agent performance\n",
    "  - Approach: Identify characteristics of top performers\n",
    "  - Business value: Improve training and territory assignment\n",
    "\n",
    "  6. Can we identify \"at-risk\" deals before they're lost?\n",
    "\n",
    "  - Evidence: We have engagement dates, close dates, and deal stages\n",
    "  - Approach: Analyze patterns in lost deals like timing, company characteristics, product mismatches\n",
    "  - Business value: Early intervention to save declining deals\n",
    "\n",
    "  7. Can we predict which \"Engaging\" deals will convert?\n",
    "\n",
    "  - Evidence: We have deals currently in \"Engaging\" status with various characteristics\n",
    "  - Approach: By building probability scores for conversion based on historical patterns\n",
    "  - Business value: Prioritize high-probability opportunities\n",
    "\n",
    "  8. Can we identify \"whale\" opportunities earlier?\n",
    "\n",
    "  - Evidence: Some deals are 10-20x larger than average (up to 30K vs 1.4K average)\n",
    "  - Approach: Build early indicators of enterprise deal potential\n",
    "  - Business value: Allocate senior resources to high-value opportunities\n",
    "\n",
    "  9. Do subsidiary relationships affect deal outcomes?\n",
    "\n",
    "  - Evidence: \"subsidiary_of\" field shows some companies are part of larger organizations\n",
    "  - Approach: Compare performance of independent vs subsidiary accounts\n",
    "  - Business value: Adjust approach for corporate vs independent buyers\n",
    "\n",
    "\n",
    "**EDA COMPLETED**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
