{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b03b303",
   "metadata": {},
   "source": [
    "### CRM Sales Opportunities Analysis - Data Integration\n",
    "\n",
    "**Business Context:** Multi-table CRM database integration<br>\n",
    "**Objective:** Merge preprocessed tables into analysis-ready master dataset<br>\n",
    "**Expected Outcome:** Clean integrated dataset ready for EDA and analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477fb62",
   "metadata": {},
   "source": [
    "#### Step 1: Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af529f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRM DATA INTEGRATION PIPELINE\n",
      "============================================================\n",
      "Combining preprocessed tables into master dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"CRM DATA INTEGRATION PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Combining preprocessed tables into master dataset\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2384df0",
   "metadata": {},
   "source": [
    "#### Step 2: Load preprocessing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3da8fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 2: LOADING PREPROCESSED TABLES\n",
      "--------------------------------------------------\n",
      "Loaded accounts: 85 rows X 7 columns\n",
      "Loaded products: 7 rows X 3 columns\n",
      "Loaded sales_pipeline: 8,800 rows X 8 columns\n",
      "Loaded sales_teams: 35 rows X 3 columns\n",
      "Loaded data_dictionary: 21 rows X 3 columns\n",
      "\n",
      "Successfully loaded 5 tables for integration\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 2: LOADING PREPROCESSED TABLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load all preprocessed tables\n",
    "data_path = Path('../data/interim/')\n",
    "tables = {}\n",
    "\n",
    "table_files = {\n",
    "    'accounts': 'accounts_preprocessed.csv',\n",
    "    'products': 'products_preprocessed.csv', \n",
    "    'sales_pipeline': 'sales_pipeline_preprocessed.csv',\n",
    "    'sales_teams': 'sales_teams_preprocessed.csv',\n",
    "    'data_dictionary': 'data_dictionary_preprocessed.csv'\n",
    "}\n",
    "\n",
    "for table_name, filename in table_files.items():\n",
    "    file_path = data_path / filename\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        tables[table_name] = df\n",
    "        print(f\"Loaded {table_name}: {df.shape[0]:,} rows X {df.shape[1]} columns\")\n",
    "        \n",
    "        # Convert datetime columns back to datetime \n",
    "        if table_name == 'sales_pipeline':\n",
    "            df['engage_date'] = pd.to_datetime(df['engage_date'])\n",
    "            df['close_date'] = pd.to_datetime(df['close_date'])\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(tables)} tables for integration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9664567",
   "metadata": {},
   "source": [
    "#### Step 3: Examine table structures and their relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c85b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: TABLE STRUCTURE ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "ACCOUNTS:\n",
      "Shape: 85 rows X 7 columns\n",
      "Columns: ['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of']\n",
      "  Primary key: account\n",
      "\n",
      "PRODUCTS:\n",
      "Shape: 7 rows X 3 columns\n",
      "Columns: ['product', 'series', 'sales_price']\n",
      "  Primary key: product\n",
      "\n",
      "SALES_PIPELINE:\n",
      "Shape: 8,800 rows X 8 columns\n",
      "Columns: ['opportunity_id', 'sales_agent', 'product', 'account', 'deal_stage', 'engage_date', 'close_date', 'close_value']\n",
      "  Key relationships: account, product, sales_agent\n",
      "\n",
      "SALES_TEAMS:\n",
      "Shape: 35 rows X 3 columns\n",
      "Columns: ['sales_agent', 'manager', 'regional_office']\n",
      "  Primary key: sales_agent\n",
      "\n",
      "DATA_DICTIONARY:\n",
      "Shape: 21 rows X 3 columns\n",
      "Columns: ['table', 'field', 'description']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 3: TABLE STRUCTURE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\n{table_name.upper()}:\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows X {df.shape[1]} columns\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show key columns for relationship identification\n",
    "    if table_name == 'sales_pipeline':\n",
    "        print(f\"  Key relationships: account, product, sales_agent\")\n",
    "    elif table_name == 'accounts':\n",
    "        print(f\"  Primary key: account\")\n",
    "    elif table_name == 'products':\n",
    "        print(f\"  Primary key: product\")\n",
    "    elif table_name == 'sales_teams':\n",
    "        print(f\"  Primary key: sales_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307e296",
   "metadata": {},
   "source": [
    "#### Step 4: Validate foreign key relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa89b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: RELATIONSHIP VALIDATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Sales Pipeline → Accounts:\n",
      "Foreign key values: 85\n",
      "Matched: 85 (100.0%)\n",
      "Unmatched: 0\n",
      "\n",
      "Sales Pipeline → Products:\n",
      "Foreign key values: 7\n",
      "Matched: 6 (85.7%)\n",
      "Unmatched: 1\n",
      "  Sample unmatched: ['GTXPro']\n",
      "\n",
      "Sales Pipeline → Sales Teams:\n",
      "Foreign key values: 30\n",
      "Matched: 30 (100.0%)\n",
      "Unmatched: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 4: RELATIONSHIP VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def validate_foreign_keys(primary_table, foreign_table, primary_key, foreign_key, relationship_name):\n",
    "    \"\"\"Validate foreign key relationships between tables\"\"\"\n",
    "    \n",
    "    # Get unique values from both tables\n",
    "    primary_values = set(primary_table[primary_key].dropna())\n",
    "    foreign_values = set(foreign_table[foreign_key].dropna())\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    total_foreign = len(foreign_values)\n",
    "    matched = len(foreign_values.intersection(primary_values))\n",
    "    unmatched = len(foreign_values - primary_values)\n",
    "    match_rate = (matched / total_foreign * 100) if total_foreign > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{relationship_name}:\")\n",
    "    print(f\"Foreign key values: {total_foreign:,}\")\n",
    "    print(f\"Matched: {matched:,} ({match_rate:.1f}%)\")\n",
    "    print(f\"Unmatched: {unmatched:,}\")\n",
    "    \n",
    "    if unmatched > 0:\n",
    "        unmatched_values = list(foreign_values - primary_values)[:5]  # Show first 5\n",
    "        print(f\"  Sample unmatched: {unmatched_values}\")\n",
    "    \n",
    "    return {\n",
    "        'relationship': relationship_name,\n",
    "        'total_foreign': total_foreign,\n",
    "        'matched': matched,\n",
    "        'unmatched': unmatched,\n",
    "        'match_rate': match_rate\n",
    "    }\n",
    "\n",
    "# Validate all key relationships\n",
    "validation_results = []\n",
    "\n",
    "# sales_pipeline → accounts\n",
    "result1 = validate_foreign_keys(\n",
    "    tables['accounts'], tables['sales_pipeline'], \n",
    "    'account', 'account', \n",
    "    \"Sales Pipeline → Accounts\"\n",
    ")\n",
    "validation_results.append(result1)\n",
    "\n",
    "# sales_pipeline → products  \n",
    "result2 = validate_foreign_keys(\n",
    "    tables['products'], tables['sales_pipeline'],\n",
    "    'product', 'product',\n",
    "    \"Sales Pipeline → Products\"\n",
    ")\n",
    "validation_results.append(result2)\n",
    "\n",
    "# sales_pipeline → sales_teams\n",
    "result3 = validate_foreign_keys(\n",
    "    tables['sales_teams'], tables['sales_pipeline'],\n",
    "    'sales_agent', 'sales_agent', \n",
    "    \"Sales Pipeline → Sales Teams\"\n",
    ")\n",
    "validation_results.append(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b5621",
   "metadata": {},
   "source": [
    "#### Step 5: Create integrated master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69b7ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 5: MASTER DATASET INTEGRATION\n",
      "--------------------------------------------------\n",
      "Starting with sales_pipeline: 8,800 rows\n",
      "Joining with accounts table...\n",
      "After joining accounts: 8,800 rows × 14 columns\n",
      "Joining with products table...\n",
      "After joining products: 8,800 rows × 16 columns\n",
      "Joining with sales_teams table...\n",
      "After joining sales_teams: 8,800 rows × 18 columns\n",
      "\n",
      "Final integrated dataset: 8,800 rows × 18 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 5: MASTER DATASET INTEGRATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Start with sales_pipeline as the primary table (contains all opportunities)\n",
    "master_df = tables['sales_pipeline'].copy()\n",
    "print(f\"Starting with sales_pipeline: {master_df.shape[0]:,} rows\")\n",
    "\n",
    "# Left join with accounts (company information)\n",
    "print(\"Joining with accounts table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['accounts'], \n",
    "    on='account', \n",
    "    how='left', \n",
    "    suffixes=('', '_account')\n",
    ")\n",
    "print(f\"After joining accounts: {master_df.shape[0]:,} rows × {master_df.shape[1]} columns\")\n",
    "\n",
    "# Left join with products (product details and pricing)\n",
    "print(\"Joining with products table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['products'],\n",
    "    on='product',\n",
    "    how='left',\n",
    "    suffixes=('', '_product')\n",
    ")\n",
    "print(f\"After joining products: {master_df.shape[0]:,} rows × {master_df.shape[1]} columns\")\n",
    "\n",
    "# Left join with sales_teams (sales agent details)\n",
    "print(\"Joining with sales_teams table...\")\n",
    "master_df = master_df.merge(\n",
    "    tables['sales_teams'],\n",
    "    on='sales_agent', \n",
    "    how='left',\n",
    "    suffixes=('', '_team')\n",
    ")\n",
    "print(f\"After joining sales_teams: {master_df.shape[0]:,} rows × {master_df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nFinal integrated dataset: {master_df.shape[0]:,} rows × {master_df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3b9e9",
   "metadata": {},
   "source": [
    "#### Step 6: Data quality assessment for integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bae72d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: INTEGRATED DATASET QUALITY ASSESSMENT\n",
      "--------------------------------------------------\n",
      "Missing values in integrated dataset:\n",
      "series: 1,480 (16.8%)\n",
      "sales_price: 1,480 (16.8%)\n",
      "close_value: 500 (5.7%)\n",
      "\n",
      "Data completeness analysis:\n",
      "Total opportunities: 8,800\n",
      "Opportunities with account data: 8,800 (100.0%)\n",
      "Opportunities with product data: 7,320 (83.2%)\n",
      "Opportunities with sales team data: 8,800 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 6: INTEGRATED DATASET QUALITY ASSESSMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Missing value analysis\n",
    "missing_summary = master_df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(master_df) * 100).round(1)\n",
    "\n",
    "print(\"Missing values in integrated dataset:\")\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_summary.index,\n",
    "    'Missing_Count': missing_summary.values,\n",
    "    'Missing_Pct': missing_pct.values\n",
    "}).query('Missing_Count > 0').sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    for _, row in missing_df.head(10).iterrows():\n",
    "        print(f\"{row['Column']}: {row['Missing_Count']:,} ({row['Missing_Pct']:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No missing values detected!\")\n",
    "\n",
    "# Data completeness by key business dimensions\n",
    "print(f\"\\nData completeness analysis:\")\n",
    "print(f\"Total opportunities: {len(master_df):,}\")\n",
    "\n",
    "# Check coverage from each joined table\n",
    "if 'revenue' in master_df.columns:\n",
    "    account_coverage = master_df['revenue'].notna().sum()\n",
    "    print(f\"Opportunities with account data: {account_coverage:,} ({account_coverage/len(master_df)*100:.1f}%)\")\n",
    "\n",
    "if 'sales_price' in master_df.columns:\n",
    "    product_coverage = master_df['sales_price'].notna().sum()\n",
    "    print(f\"Opportunities with product data: {product_coverage:,} ({product_coverage/len(master_df)*100:.1f}%)\")\n",
    "\n",
    "if 'manager' in master_df.columns:\n",
    "    team_coverage = master_df['manager'].notna().sum()\n",
    "    print(f\"Opportunities with sales team data: {team_coverage:,} ({team_coverage/len(master_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c06c30",
   "metadata": {},
   "source": [
    "#### Step 7: Integration validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16d23309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: INTEGRATION VALIDATION\n",
      "--------------------------------------------------\n",
      "Data integrity checks:\n",
      "Duplicate opportunities: 0 (should be 0)\n",
      "Account data joined: 100.0% of opportunities\n",
      "Product data joined: 83.2% of opportunities\n",
      "Team data joined: 100.0% of opportunities\n",
      "Won deals with values: 4238/4238 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 7: INTEGRATION VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Validate data integrity\n",
    "print(\"Data integrity checks:\")\n",
    "\n",
    "# Check for unexpected duplicates\n",
    "duplicate_opps = master_df['opportunity_id'].duplicated().sum()\n",
    "print(f\"Duplicate opportunities: {duplicate_opps} (should be 0)\")\n",
    "\n",
    "# Check join effectiveness  \n",
    "total_ops = len(master_df)\n",
    "\n",
    "if 'revenue' in master_df.columns:\n",
    "    account_join_success = (master_df['revenue'].notna().sum() / total_ops * 100)\n",
    "    print(f\"Account data joined: {account_join_success:.1f}% of opportunities\")\n",
    "\n",
    "if 'sales_price' in master_df.columns:\n",
    "    product_join_success = (master_df['sales_price'].notna().sum() / total_ops * 100) \n",
    "    print(f\"Product data joined: {product_join_success:.1f}% of opportunities\")\n",
    "\n",
    "if 'manager' in master_df.columns:\n",
    "    team_join_success = (master_df['manager'].notna().sum() / total_ops * 100)\n",
    "    print(f\"Team data joined: {team_join_success:.1f}% of opportunities\")\n",
    "\n",
    "# Business logic validation\n",
    "if 'deal_stage' in master_df.columns and 'close_value' in master_df.columns:\n",
    "    won_deals_with_value = master_df[\n",
    "        (master_df['deal_stage'] == 'Won') & \n",
    "        (master_df['close_value'].notna())\n",
    "    ].shape[0]\n",
    "    total_won_deals = (master_df['deal_stage'] == 'Won').sum()\n",
    "    \n",
    "    if total_won_deals > 0:\n",
    "        print(f\"Won deals with values: {won_deals_with_value}/{total_won_deals} ({won_deals_with_value/total_won_deals*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdf3b5",
   "metadata": {},
   "source": [
    "#### Step 8: Display a sample of integrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8519a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 8: INTEGRATED DATASET PREVIEW\n",
      "--------------------------------------------------\n",
      "Sample of integrated data:\n",
      "  opportunity_id      sales_agent         product  account deal_stage  \\\n",
      "0       1C1I7A6R      Moses Frase  GTX Plus Basic  Cancity        Won   \n",
      "1       Z063OYW0  Darcel Schlecht          GTXPro    Isdom        Won   \n",
      "2       EC4QE1BX  Darcel Schlecht      MG Special  Cancity        Won   \n",
      "3       MV1LWRNH      Moses Frase       GTX Basic  Codehow        Won   \n",
      "4       PE84CX4O        Zane Levy       GTX Basic   Hatfan        Won   \n",
      "\n",
      "  engage_date close_date  close_value    sector  year_established  revenue  \\\n",
      "0  2016-10-20 2017-03-01       1054.0    retail              2001   718.62   \n",
      "1  2016-10-25 2017-03-11       4514.0   medical              2002  3178.24   \n",
      "2  2016-10-25 2017-03-07         50.0    retail              2001   718.62   \n",
      "3  2016-10-25 2017-03-09        588.0  software              1998  2714.90   \n",
      "4  2016-10-25 2017-03-02        517.0  services              1982   792.46   \n",
      "\n",
      "   employees office_location     subsidiary_of series  sales_price  \\\n",
      "0       2448   United States       Independent    GTX       1096.0   \n",
      "1       4540   United States       Independent    NaN          NaN   \n",
      "2       2448   United States       Independent     MG         55.0   \n",
      "3       2641   United States  Acme Corporation    GTX        550.0   \n",
      "4       1299   United States       Independent    GTX        550.0   \n",
      "\n",
      "            manager regional_office  \n",
      "0  Dustin Brinkmann         Central  \n",
      "1     Melvin Marxen         Central  \n",
      "2     Melvin Marxen         Central  \n",
      "3  Dustin Brinkmann         Central  \n",
      "4     Summer Sewald            West  \n",
      "\n",
      "Dataset info:\n",
      "Shape: (8800, 18)\n",
      "\n",
      "Column summary:\n",
      "opportunity_id: 8,800/8,800 non-null (object)\n",
      "sales_agent: 8,800/8,800 non-null (object)\n",
      "product: 8,800/8,800 non-null (object)\n",
      "account: 8,800/8,800 non-null (object)\n",
      "deal_stage: 8,800/8,800 non-null (object)\n",
      "engage_date: 8,800/8,800 non-null (datetime64[ns])\n",
      "close_date: 8,800/8,800 non-null (datetime64[ns])\n",
      "close_value: 8,300/8,800 non-null (float64)\n",
      "sector: 8,800/8,800 non-null (object)\n",
      "year_established: 8,800/8,800 non-null (int64)\n",
      "revenue: 8,800/8,800 non-null (float64)\n",
      "employees: 8,800/8,800 non-null (int64)\n",
      "office_location: 8,800/8,800 non-null (object)\n",
      "subsidiary_of: 8,800/8,800 non-null (object)\n",
      "series: 7,320/8,800 non-null (object)\n",
      "sales_price: 7,320/8,800 non-null (float64)\n",
      "manager: 8,800/8,800 non-null (object)\n",
      "regional_office: 8,800/8,800 non-null (object)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 8: INTEGRATED DATASET PREVIEW\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Sample of integrated data:\")\n",
    "print(master_df.head())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"Shape: {master_df.shape}\")\n",
    "\n",
    "print(f\"\\nColumn summary:\")\n",
    "for col in master_df.columns:\n",
    "    non_null = master_df[col].notna().sum()\n",
    "    data_type = master_df[col].dtype\n",
    "    print(f\"{col}: {non_null:,}/{len(master_df):,} non-null ({data_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c29a6",
   "metadata": {},
   "source": [
    "#### Step 9: Save the integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "751f5031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 8: SAVING INTEGRATED DATASET\n",
      "--------------------------------------------------\n",
      "Saved crm_master_dataset.csv: 8,800 rows X 18 columns\n",
      "Saved integration_summary.csv\n",
      "\n",
      "============================================================\n",
      "DATA INTEGRATION COMPLETED\n",
      "============================================================\n",
      "Master dataset: 8,800 opportunities × 18 features\n",
      "Integration success rates:\n",
      "Sales Pipeline → Accounts: 100.0%\n",
      "Sales Pipeline → Products: 85.7%\n",
      "Sales Pipeline → Sales Teams: 100.0%\n",
      "Output location: ../data/processed\n",
      "Ready for exploratory data analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 8: SAVING INTEGRATED DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create processed data directory\n",
    "output_path = Path('../data/processed/')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save master integrated dataset\n",
    "master_filename = 'crm_master_dataset.csv'\n",
    "master_filepath = output_path / master_filename\n",
    "master_df.to_csv(master_filepath, index=False)\n",
    "print(f\"Saved {master_filename}: {master_df.shape[0]:,} rows X {master_df.shape[1]} columns\")\n",
    "\n",
    "# Save integration summary\n",
    "integration_summary = {\n",
    "    'metric': [\n",
    "        'Total Opportunities',\n",
    "        'Total Features',\n",
    "        'Original Tables Integrated',\n",
    "        'Account Data Coverage',\n",
    "        'Product Data Coverage', \n",
    "        'Sales Team Data Coverage',\n",
    "        'Complete Records'\n",
    "    ],\n",
    "    'value': [\n",
    "        len(master_df),\n",
    "        master_df.shape[1],\n",
    "        len(tables) - 1,  \n",
    "        master_df['revenue'].notna().sum() if 'revenue' in master_df.columns else 0,\n",
    "        master_df['sales_price'].notna().sum() if 'sales_price' in master_df.columns else 0,\n",
    "        master_df['manager'].notna().sum() if 'manager' in master_df.columns else 0,\n",
    "        master_df.dropna().shape[0]\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(integration_summary)\n",
    "summary_df.to_csv(output_path / 'integration_summary.csv', index=False)\n",
    "print(f\"Saved integration_summary.csv\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA INTEGRATION COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Master dataset: {master_df.shape[0]:,} opportunities × {master_df.shape[1]} features\")\n",
    "print(f\"Integration success rates:\")\n",
    "for result in validation_results:\n",
    "    print(f\"{result['relationship']}: {result['match_rate']:.1f}%\")\n",
    "print(f\"Output location: {output_path}\")\n",
    "print(\"Ready for exploratory data analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
